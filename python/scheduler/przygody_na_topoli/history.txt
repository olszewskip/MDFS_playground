## 25.02.2019

## locally (via VPN_ICM)
ssh po276940@login.icm.edu.pl
# >> password

## @hpc
mkdir my_virtualenvs
cd my_virtualenvs
mkdir MDFS
cd MDFS

srun -N 1 -p topola -n 1 --pty bash -l
module load common/mpi/mpich/3.2
which mpirun  # /apps/common/mpich/3.2/INTEL/bin/mpirun
module load common/python/3.5.2
which python3  # /mnt/software/common/python/3.5.2/bin/python3
virtualenv example
cd example
source bin/activate
which python  # ~/my_virtualenvs/MDFS/example/bin/python
pip install numpy, scipy, mpi4py, pybind11, cppimport
mkdir src
cd src

## locally (via VPN_ICM)
pwd  # .../git-repos/MDFS_playground/python/scheduler/przygody_na_topoli
scp dummy_work1.cpp dummy_work1_parallel.py po276940@login.icm.edu.pl:~/my_virtualenvs/MDFS/example/src

## @hpc
ls src  # dummy_work1.cpp  parallel_dummy_work1.py
python
>>> import cppimport; cppimport.imp("dummy_work1"); quit()
ls  # dummy_work1.cpp  gmon.out  dummy_work1.cpython-35m-x86_64-linux-gnu.so  parallel_dummy_work1.py
exit

srun -N 4 -p topola -n 16 --cpu_bind=none --pty bash -l
module load common/mpi/mpich/3.2
module load common/python/3.5.2
cd my_virtualenvs/MDFS/example/src
source ../bin/activate
mpirun -n 1 python dummy_work1_parallel.py  #  1 x 4 threads = 8.71 sec
mpirun -n 2 python dummy_work1_parallel.py  #  2 x 4 threads = 4.43 sec
mpirun -n 4 python dummy_work1_parallel.py  #  4 x 4 threads = 2.35 sec

# change value of the NUM_THREADS in the cpp module from 4 to 2, recompile
mpirun -n 1 python dummy_work1_parallel.py  #  1 x 2 threads = 8.60 sec
mpirun -n 2 python dummy_work1_parallel.py  #  2 x 2 threads = 4.40 sec
mpirun -n 4 python dummy_work1_parallel.py  #  4 x 2 threads = 2.35 sec


## 26.02.2019

## ask mpi4py what value of the MPI_Query_thread variable does it see (https://www.mpich.org/static/docs/v3.1/www3/MPI_Query_thread.html):

## @hpc
cd my_virtualenvs/MDFS/example/src
srun -N 1 -p topola -n 8 --cpu_bind=none --pty bash -l
module load common/mpi/mpich/3.2
module load common/python/3.5.2
source ../bin/activate
python
>>> from mpi4py import MPI
>>> MPI.Query_thread()  # returns 3  <- ?? MPI_THREAD_MULTIPLE (Multiple threads may call MPI, with no restrictions)

## take a basic omp example (calculation of pi) called via pybind and check times (no mpi this time)

## locally (via VPN_ICM)
pwd  # .../git-repos/MDFS_playground/python/scheduler/przygody_na_topoli
scp dummy_work_pi.cpp dummy_work_pi_n1.py po276940@login.icm.edu.pl:~/my_virtualenvs/MDFS/example/src
## @hpc
cd my_virtualenvs/MDFS/example/src
srun -N 4 -p topola -n 16 --cpu_bind=none --cpus-per-task=4 --pty bash -l
module load common/mpi/mpich/3.2
module load common/python/3.5.2
source ../bin/activate
python
>>> import cppimport; cppimport.imp("dummy_work_pi"); quit()

export OMP_NUM_THREADS=1
python dummy_work_pi_n1.py   # 7.40 sec
export OMP_NUM_THREADS=2
python dummy_work_pi_n1.py   # 3.71 sec
export OMP_NUM_THREADS=4
python dummy_work_pi_n1.py   # 1.85 sec
export OMP_NUM_THREADS=8
python dummy_work_pi_n1.py   # 0.93 sec
export OMP_NUM_THREADS=16
python dummy_work_pi_n1.py   # 0.46 sec
export OMP_NUM_THREADS=32
python dummy_work_pi_n1.py   # 0.39 sec
export OMP_NUM_THREADS=64
python dummy_work_pi_n1.py   # 0.39 sec

## 27.02.2019

## take the very same pi example as above but with mpi4py added on top of the pybind11-enabled call to cpp function that utilizes omp

## locally (via VPN_ICM)
pwd  # .../git-repos/MDFS_playground/python/scheduler/przygody_na_topoli
scp dummy_work_pi_parallel.py po276940@login.icm.edu.pl:~/my_virtualenvs/MDFS/example/src
## @hpc
cd my_virtualenvs/MDFS/example/src
srun -N 4 -p topola -n 16 --cpu_bind=none --cpus-per-task=4 --pty bash -l
module load common/mpi/mpich/3.2
module load common/python/3.5.2
source ../bin/activate

export OMP_NUM_THREADS=1
mpirun -n 1 python dummy_work_pi_parallel.py  # 7.40 sec
export OMP_NUM_THREADS=2
mpirun -n 1 python dummy_work_pi_parallel.py  # 3.70 sec
export OMP_NUM_THREADS=4
mpirun -n 1 python dummy_work_pi_parallel.py  # 1.85 sec
export OMP_NUM_THREADS=8
mpirun -n 1 python dummy_work_pi_parallel.py  # 1.87 sec

export OMP_NUM_THREADS=1
mpirun -n 2 python dummy_work_pi_parallel.py  # 3.70 sec
export OMP_NUM_THREADS=2
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.85 sec
export OMP_NUM_THREADS=4
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.85 sec

export OMP_NUM_THREADS=1
mpirun -n 4 python dummy_work_pi_parallel.py  # 1.85 sec
export OMP_NUM_THREADS=2
mpirun -n 4 python dummy_work_pi_parallel.py  # 1.86 sec


## 28.02

## repeat yesterdays's experiment with different values passed to the flags in 'srun' call

srun -N 4 -p topola -n 4 --cpu_bind=none --cpus-per-task=16 --pty bash -l

export OMP_NUM_THREADS=1
mpirun -n 1 python dummy_work_pi_parallel.py  # 7.40 sec
export OMP_NUM_THREADS=2
mpirun -n 1 python dummy_work_pi_parallel.py  # 3.70 sec
export OMP_NUM_THREADS=4
mpirun -n 1 python dummy_work_pi_parallel.py  # 1.85 sec
export OMP_NUM_THREADS=8
mpirun -n 1 python dummy_work_pi_parallel.py  # 0.93 sec
export OMP_NUM_THREADS=16
mpirun -n 1 python dummy_work_pi_parallel.py  # 0.69 sec
export OMP_NUM_THREADS=32
mpirun -n 1 python dummy_work_pi_parallel.py  # 0.65 sec
export OMP_NUM_THREADS=64
mpirun -n 1 python dummy_work_pi_parallel.py  # 0.62 sec

export OMP_NUM_THREADS=1
mpirun -n 2 python dummy_work_pi_parallel.py  # 3.70 sec
export OMP_NUM_THREADS=2
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.85 sec
export OMP_NUM_THREADS=4
mpirun -n 2 python dummy_work_pi_parallel.py  # 0.93 sec
export OMP_NUM_THREADS=8
mpirun -n 2 python dummy_work_pi_parallel.py  # 0.69 sec
export OMP_NUM_THREADS=16
mpirun -n 2 python dummy_work_pi_parallel.py  # 0.36 sec
export OMP_NUM_THREADS=32
mpirun -n 2 python dummy_work_pi_parallel.py  # 0.34 sec

export OMP_NUM_THREADS=1
mpirun -n 4 python dummy_work_pi_parallel.py  # 1.85 sec
export OMP_NUM_THREADS=2
mpirun -n 4 python dummy_work_pi_parallel.py  # 0.93 sec
export OMP_NUM_THREADS=4
mpirun -n 4 python dummy_work_pi_parallel.py  # 0.46 sec
export OMP_NUM_THREADS=8
mpirun -n 4 python dummy_work_pi_parallel.py  # 0.23 sec
export OMP_NUM_THREADS=16
mpirun -n 4 python dummy_work_pi_parallel.py  # 0.19 sec
export OMP_NUM_THREADS=32
mpirun -n 4 python dummy_work_pi_parallel.py  # 0.19 sec


srun -N 4 -p topola -n 4 --cpu_bind=none --cpus-per-task=8 --pty bash -l

export OMP_NUM_THREADS=1
mpirun -n 1 python dummy_work_pi_parallel.py  # 7.40 sec
export OMP_NUM_THREADS=2
mpirun -n 1 python dummy_work_pi_parallel.py  # 3.70 sec
export OMP_NUM_THREADS=4
mpirun -n 1 python dummy_work_pi_parallel.py  # 1.85 sec
export OMP_NUM_THREADS=8
mpirun -n 1 python dummy_work_pi_parallel.py  # 0.93 sec
export OMP_NUM_THREADS=16
mpirun -n 1 python dummy_work_pi_parallel.py  # 0.97 sec

export OMP_NUM_THREADS=1
mpirun -n 2 python dummy_work_pi_parallel.py  # 3.70 sec
export OMP_NUM_THREADS=2
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.85 sec
export OMP_NUM_THREADS=4
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.38 sec
export OMP_NUM_THREADS=8
mpirun -n 2 python dummy_work_pi_parallel.py  # 0.93 sec
export OMP_NUM_THREADS=16
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.11 sec


srun --nodes=4 --ntasks=16 --cpu_bind=threads --cpus-per-task=4 -p topola --time=30 --pty bash -l

export OMP_NUM_THREADS=4
mpirun -n 4 python dummy_work_pi_parallel.py  # 1.87 sec
export OMP_NUM_THREADS=2
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.85 sec


srun --nodes=4 --ntasks=16 --cpu_bind=none --cpus-per-task=4 -p topola --time=30 --pty bash -l

export OMP_NUM_THREADS=4
mpirun -n 4 python dummy_work_pi_parallel.py  # 1.87 sec
export OMP_NUM_THREADS=2
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.85 sec


srun --nodes=2 --ntasks=8 --cpu_bind=none --cpus-per-task=4 -p topola --time=10 --pty bash -l
export OMP_NUM_THREADS=4
mpirun -n 4 python dummy_work_pi_parallel.py  # 1.41 sec
mpirun -n 2 python dummy_work_pi_parallel.py  # 0.93 sec
export OMP_NUM_THREADS=2
mpirun -n 4 python dummy_work_pi_parallel.py  # 1.61 sec
mpirun -n 2 python dummy_work_pi_parallel.py  # 1.85 sec


srun --nodes=4 --cpu_bind=none --cpus-per-task=8 -p topola --time=10 --pty bash -l

export OMP_NUM_THREADS=1
mpirun -n  2 python dummy_work_pi_parallel.py  #  3.70 sec
mpirun -n  4 python dummy_work_pi_parallel.py  #  1.83 sec
mpirun -n  8 python dummy_work_pi_parallel.py  #  0.92 sec
mpirun -n 16 python dummy_work_pi_parallel.py  #  0.46 sec
mpirun -n 32 python dummy_work_pi_parallel.py  #  0.30 sec
mpirun -n 64 python dummy_work_pi_parallel.py  #  0.40 sec

export OMP_NUM_THREADS=2
mpirun -n  2 python dummy_work_pi_parallel.py  #  1.85 sec
mpirun -n  4 python dummy_work_pi_parallel.py  #  0.93 sec
mpirun -n  8 python dummy_work_pi_parallel.py  #  0.70 sec
mpirun -n 16 python dummy_work_pi_parallel.py  #  0.75 sec

export OMP_NUM_THREADS=4
mpirun -n  2 python dummy_work_pi_parallel.py  #  1.39 sec
mpirun -n  4 python dummy_work_pi_parallel.py  #  0.70 sec
mpirun -n  8 python dummy_work_pi_parallel.py  #  0.59 sec
mpirun -n 16 python dummy_work_pi_parallel.py  #  0.62 sec

export OMP_NUM_THREADS=8
mpirun -n  2 python dummy_work_pi_parallel.py  #  0.93 sec
mpirun -n  4 python dummy_work_pi_parallel.py  #  0.47 sec
mpirun -n  8 python dummy_work_pi_parallel.py  #  0.66 sec

export OMP_NUM_THREADS=16
mpirun -n  2 python dummy_work_pi_parallel.py  #  1.12 sec
mpirun -n  4 python dummy_work_pi_parallel.py  #  0.57 sec
mpirun -n  8 python dummy_work_pi_parallel.py  #  0.47 sec


## 3.03

## try the informarmation-gain calculation

# @locally
scp infogain.cpp infogain.py matrix_class.h madelon_tiny.csv po276940@login.icm.edu.pl:~/my_virtualenvs/MDFS/example/src

#
srun -N 4 -p topola -n 16 --cpu_bind=none --cpus-per-task=4 --time=25 --pty bash -l
...
export OMP_NUM_THREADS=1
mpirun -n  2 python infogain.py  # 7.76 sec
mpirun -n  3 python infogain.py  # 3.87 sec
mpirun -n  5 python infogain.py  # 2.67 sec
mpirun -n  9 python infogain.py  # 1.72 sec
mpirun -n 17 python infogain.py  # 1.12 sec

export OMP_NUM_THREADS=2
mpirun -n  2 python infogain.py  # 15.89 sec
mpirun -n  3 python infogain.py  # 14.15 sec

export OMP_NUM_THREADS=4
mpirun -n  2 python infogain.py  # 142.72 sec
mpirun -n  3 python infogain.py  # 107.36 sec

scp po276940@login.icm.edu.pl:~/my_virtualenvs/MDFS/example/src/final_results.pkl

